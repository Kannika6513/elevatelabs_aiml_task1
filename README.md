# ğŸš€ AI/ML Internship â€“ **Elevate Labs**  
ğŸ“‚ *Repository for Internship Tasks & Projects*  

---

## ğŸ“œ Overview  
This repository contains a collection of **Machine Learning** tasks and projects completed during my internship at **Elevate Labs**.  
Each task focuses on different aspects of **data preprocessing, exploratory data analysis, model building, and evaluation**.  

---

## ğŸ“‘ Contents  

1. ğŸš¢ **Titanic Dataset â€“ Data Cleaning, Preprocessing & Visualization**  
2. ğŸ“Š **Titanic Dataset â€“ Exploratory Data Analysis (EDA)**  
3. ğŸ  **House Price Prediction â€“ Linear Regression**  
4. ğŸ©º **Breast Cancer Classification â€“ Logistic Regression**  
5. â¤ï¸ **Heart Disease Prediction â€“ Decision Tree & Random Forest**  

---

## ğŸš¢ **Task 1 â€“ Titanic Dataset (Data Cleaning & Preprocessing)**  

**ğŸ“ Dataset:** Passenger details (age, gender, fare, family size, etc.) to predict survival chances.  

**ğŸ”§ Steps Performed:**  
- ğŸ—‚ **Data Loading** â€“ Read dataset using `pandas`.  
- ğŸ©¹ **Handling Missing Values:**  
  - Filled `Age` with median.  
  - Filled `Embarked` with mode.  
  - Filled `Cabin` with `"Unknown"`.  
  - Filled `Fare` with median.  
- ğŸ—‘ **Dropped Unnecessary Columns:** `Name`, `Ticket`.  
- ğŸ”¤ **Encoding Categorical Variables:** Applied one-hot encoding to `Sex` & `Embarked`.  
- ğŸ“‰ **Outlier Detection & Removal:** Used IQR method on `Age`, `Fare`, `SibSp`, `Parch`.  
- ğŸ“ **Feature Scaling:** Standardized numeric features using `StandardScaler`.  
- ğŸ“Š **Visualization:** Boxplots before & after outlier removal.  

**ğŸ“¦ Libraries:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `sklearn`  

---

## ğŸ“Š **Task 2 â€“ Titanic Dataset (Exploratory Data Analysis)**  

**ğŸ¯ Objective:** Explore patterns & relationships in the Titanic dataset.  

**ğŸ“Œ Highlights:**  
- ğŸ“ˆ **Summary Statistics:** Mean, median, standard deviation, etc.  
- ğŸ“Š **Visual Analysis:**  
  - Histograms & boxplots for `Age`, `Fare`  
  - Bar plots for `Pclass` & `Sex` vs `Survived`  
  - Correlation matrix & pairplot for feature relationships  

**ğŸ§  Key Inferences:**  
- ğŸ‘© Women had a higher survival rate.  
- ğŸ›³ First-class passengers had better survival chances.  
- ğŸ’° Fare distribution was right-skewed with outliers.  
- ğŸ“‰ Negative correlation between `Pclass` and `Fare`.  

---

## ğŸ  **Task 3 â€“ House Price Prediction (Linear Regression)**  

**ğŸ“ Dataset:** `Housing.csv` â€“ 545 records, 13 features.  

**ğŸ“Œ Steps:**  
1. ğŸ“¥ Import & preprocess data (encode categorical variables).  
2. âœ‚ Split into train/test sets.  
3. ğŸ— Train a `LinearRegression` model.  
4. ğŸ“ Evaluate using MAE, MSE, RÂ² score.  
5. ğŸ“Š Plot predicted vs actual prices.  

**ğŸ“Š Sample Results:**  
- MAE: ~â‚¹9.7 Lakhs  
- RÂ² Score: ~0.65  
- Moderate fit with interpretable coefficients.  

---

## ğŸ©º **Task 4 â€“ Breast Cancer Classification (Logistic Regression)**  

**ğŸ“Œ Steps:**  
- ğŸ—‘ Drop unnecessary columns: `id`, `Unnamed: 32`  
- ğŸ”¤ Encode target (`M` â†’ 1, `B` â†’ 0)  
- âœ‚ Train/test split (80/20) & standardize features  
- ğŸ— Train `LogisticRegression` model  
- ğŸ“Š Evaluate with confusion matrix, precision, recall, ROC-AUC  
- âš¡ Threshold tuning example at 0.3  
- ğŸ“ˆ Sigmoid curve plot & explanation  

---

## â¤ï¸ **Task 5 â€“ Heart Disease Prediction (Decision Tree & Random Forest)**  

**ğŸ“Œ Project Overview:**  
Uses `heart.csv` to predict heart disease. Models used:  
- ğŸŒ³ Decision Tree Classifier  
- ğŸŒ² Random Forest Classifier  

**ğŸ›  Steps:**  
1. Train & visualize Decision Tree (controlled depth to avoid overfitting).  
2. Analyze overfitting by adjusting max depth.  
3. Train Random Forest & compare accuracy.  
4. Interpret feature importances.  
5. Evaluate both models using cross-validation.  

---
# ğŸŒ¸ TASK 6 KNN Classification on Iris Dataset ğŸŒ¼

## ğŸŒº Steps Performed
1. **Dataset**  
   - Used `Iris.csv` dataset.  
   - Dropped ID column.  
   - Encoded species labels into numbers.  
   - Normalized features using `StandardScaler`.  

2. **ğŸŒ» Model**  
   - Applied `KNeighborsClassifier` from `sklearn`.  
   - Tested different `K` values to find the best accuracy.  

3. **ğŸŒ¹ Evaluation**  
   - Computed accuracy score.  
   - Created confusion matrix.  

4. **ğŸŒ· Visualization**  
   - Plotted decision boundaries using first two normalized features.  
   - Colors: â¤ï¸ (Setosa), ğŸ’š (Versicolor), ğŸ’™ (Virginica).  

## ğŸŒ¼ Output
- Best K value  
- Accuracy score  
- Confusion matrix plot  
- Decision boundary plot

# ğŸ— Breast Cancer Classification

## ğŸ“Œ Task
Train a machine learning model on the **Breast Cancer Wisconsin dataset** ğŸ©º and visualize **decision boundaries** ğŸŒˆ for two selected features.

## ğŸ“‚ Dataset
- Source: `sklearn.datasets.load_breast_cancer()`
- Target: Malignant (0) / Benign (1)

## ğŸ›  Steps
1ï¸âƒ£ Import libraries  
2ï¸âƒ£ Load & inspect dataset  
3ï¸âƒ£ Select 2 features for plotting  
4ï¸âƒ£ Split into train/test sets  
5ï¸âƒ£ Train classifier (e.g., Logistic Regression, SVM, KNN)  
6ï¸âƒ£ Plot decision boundaries & accuracy score  

## ğŸš€ Output
- Graph with decision boundary separation ğŸ–Œ  
- Accuracy printed in console ğŸ“ˆ
